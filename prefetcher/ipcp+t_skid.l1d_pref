/***************************************************************************
For the Third Data Prefetching Championship - DPC3

Paper ID: #4
Instruction Pointer Classifying Prefetcher - IPCP

Authors: 
Samuel Pakalapati - samuelpakalapati@gmail.com
Biswabandan Panda - biswap@cse.iitk.ac.in
***************************************************************************/

#include "cache.h"

#define NUM_IP_TABLE_L1_ENTRIES 1024                        // IP table entries 
#define NUM_GHB_ENTRIES 16                                  // Entries in the GHB
#define NUM_IP_INDEX_BITS 10                                // Bits to index into the IP table 
#define NUM_IP_TAG_BITS 6                                   // Tag bits per IP table entry
#define S_TYPE 1                                            // stream
#define CS_TYPE 2                                           // constant stride
#define CPLX_TYPE 3                                         // complex stride
#define NL_TYPE 4                                           // next line
#define PREFETCH_DEGREE 3                                   // intial prefetch degree
#define THRESHOLD 7                                         // maximum prefetch degree
#define AC_HIGH 0.75                                        // higher threshold
#define AC_LOW 0.40                                         // lower threshold

// #define SIG_DEBUG_PRINT
#ifdef SIG_DEBUG_PRINT
#define SIG_DP(x) x
#else
#define SIG_DP(x)
#endif

#include <array>
#include <algorithm>

// ====================================
//   T-SKID specific term definition
// ====================================
// trigger: cache accesses that occurs at a good time to prefetch
// step: the delta between two access addresses issued from the same PC

namespace { // anonymous

// PC
// This is a strong typedef of uint16_t.
// PC is 25-bit in ChampSim.
// PC is folded into 16-bit.
class PC {
	uint16_t value = 0;
public:
	PC() = default;
	PC(const PC&) = default;
	PC(PC&&) = default;
	PC& operator=(const PC&) = default;
	PC& operator=(PC&&) = default;
	explicit PC(uint64_t pc) noexcept : value((pc^(pc>>16))) {}

	bool operator==(PC rhs) const noexcept { return value == rhs.value; }
	bool operator!=(PC rhs) const noexcept { return value != rhs.value; }
	bool operator<(PC rhs) const { return value < rhs.value; }
	explicit operator uint64_t() const noexcept { return value; }
};

// RawAddr
// This is a strong typedef of uint64_t.
// RawAddr is 48-bit in ChampSim.
class RawAddr {
	uint64_t value = 0;
public:
	RawAddr() = default;
	RawAddr(const RawAddr&) = default;
	RawAddr(RawAddr&&) = default;
	RawAddr& operator=(const RawAddr&) = default;
	RawAddr& operator=(RawAddr&&) = default;
	explicit RawAddr(uint64_t addr) noexcept : value(addr) {}

	bool operator==(RawAddr rhs) const noexcept { return value == rhs.value; }
	bool operator!=(RawAddr rhs) const noexcept { return value != rhs.value; }
	explicit operator uint64_t() const noexcept { return value; }
	RawAddr operator+(int64_t d) const noexcept { return RawAddr(value + d); }
	int64_t operator-(RawAddr rhs) const noexcept { return value - rhs.value; }
};

RawAddr mask_line_offset(RawAddr addr) noexcept {
	return RawAddr( static_cast<uint64_t>(addr) & -BLOCK_SIZE );
}

bool is_in_the_same_page(RawAddr addr1, RawAddr addr2) noexcept {
	return static_cast<uint64_t>(addr1) >> LOG2_PAGE_SIZE == static_cast<uint64_t>(addr2) >> LOG2_PAGE_SIZE;
}

int count_pref;

// -------------
//  Delay queue
// -------------
// This implementation of the delay queue is specific to the DPC3
// simulator, as the DPC3 prefetcher can act only at certain clock
// cycles. In a real processor, the delay queue implementation
// can be simpler.
// (http://comparch-conf.gatech.edu/dpc2/resource/dpc2_michaud.c)

template<class T, size_t N, uint16_t Delay>
class DelayQueue {
	std::array<T, N> table;
	std::array<uint16_t, N> cycle;
	size_t tail;
	size_t head;
	// For distinguish whether full or empty when tail == head
	bool full;
public:
	DelayQueue() noexcept : tail(0), head(0), full(false) {}

	void push(T elem, uint16_t current_cycle) {
		if (full) {
			// When the queue is full, only to do is trash elem.
			return;
		} else {
			table[tail] = std::move(elem);
			cycle[tail] = current_cycle;
			tail = (tail + 1) % N;
			if (tail == head) { full = true; }
		}
	}

	bool ready_to_pop(uint16_t current_cycle) const noexcept {
		const bool empty = !full && tail == head;
		if (empty) {
			return false;
		} else {
			const uint16_t push_cycle = cycle[head];
			const uint16_t ready_cycle = static_cast<uint16_t>(cycle[head] + Delay);
			if (push_cycle < ready_cycle) {
				// 0        push    ready      65536
				// |          |       |          |
				// <--ready-->        <--ready-->
				return current_cycle < push_cycle || ready_cycle <= current_cycle;
			} else {
				// 0 ready                push 65536
				// |   |                    |    |
				//     <-------ready------->
				return current_cycle < push_cycle && ready_cycle <= current_cycle;
			}
		}
	}

	const T& front() const noexcept {
		return table[head];
	}

	void pop() noexcept {
		head = (head + 1) % N;
		full = false;
	}
};

// -------------------------
//  Fully associative table
// -------------------------
// In N-SKID prefetcher, this template is used for Inflight Prefetch
// Table. Since there is a theoretical upper limit to the number of
// entries in the table, no information for replacement is needed.
template<class T, class U, size_t N>
class FullyAssociativeTable {
	struct Entry {
		T key;
		U value;
		bool valid;
	};
	std::array<Entry, N> table;
public:
	FullyAssociativeTable() : table{} {}
	void insert(T tag, U elem) {
		const auto existing_entry = std::find_if(table.begin(), table.end(), [&tag](const Entry& entry) { return entry.valid && entry.key == tag; });
		if (existing_entry != table.end()) {
			existing_entry->value = std::move(elem);
		} else {
			const auto invalid_entry = std::find_if(table.begin(), table.end(), [](const Entry& entry) noexcept { return !entry.valid; });
			if (invalid_entry != table.end()) {
				*invalid_entry = Entry { std::move(tag), std::move(elem), true };
			}
		}
	}
	bool contains(const T& tag) const {
		return std::any_of(table.begin(), table.end(), [&tag](const Entry& entry) { return entry.valid && entry.key == tag; });
	}
	const U& get(const T& tag) const {
		assert(contains(tag));
		const auto existing_entry = std::find_if(table.begin(), table.end(), [&tag](const Entry& entry) { return entry.valid && entry.key == tag; });
		return existing_entry->value;
	}
	void invalidate(const T& tag) {
		assert(contains(tag));
		const auto existing_entry = std::find_if(table.begin(), table.end(), [&tag](const Entry& entry) { return entry.valid && entry.key == tag; });
		existing_entry->valid = false;
	}
};

// ---------------------------
//  Fully associative LRU set
// ---------------------------
// In N-SKID prefetcher, this template is used for Recent Requests Table
// and Trigger Table entry. This performs LRU replacement using lru_order.
template<class T, size_t N>
class FullyAssociativeLRUSet {
	std::array<T, N> table;
	std::array<size_t, N> lru_order;
	void touch(size_t pos) {
		for (size_t i = 0; i < N; ++i) {
			if (lru_order[i] < lru_order.at(pos)) {
				++lru_order[i];
			}
		}
		lru_order.at(pos) = 0;
	}
public:
	FullyAssociativeLRUSet() : table{} {
		for (size_t i = 0; i < N; ++i) {
			lru_order[i] = i;
		}
	}

	const T* begin() const { return table.begin(); }
	T* begin() { return table.begin(); }
	const T* end() const { return table.end(); }
	T* end() { return table.end(); }
	void insert(const T& val) {
		const size_t lru_pos = std::max_element(lru_order.begin(), lru_order.end() ) - lru_order.begin();
		table.at(lru_pos) = val;
		touch(lru_pos);
	}

	void insert_or_touch(const T& val) {
		const size_t existing_pos = std::find(begin(), end(), val) - begin();
		if (existing_pos != N) {
			touch(existing_pos);
		} else {
			insert(val);
		}
	}
	bool contains(const T& val) const { return std::find(begin(), end(), val) != end(); }
	void move_to_lru(const T& val) {
		const auto existing_pos = std::find(begin(), end(), val) - begin();
		lru_order.at(existing_pos) = N - 1;
	}
};

// ------------------------------
//  PC set associative LRU table
// ------------------------------
// In N-SKID prefetcher, this template is used for Trigger Table and
// Step Table. This performs LRU replacement using lru_order.
template<class U, size_t N_Sets, size_t N_Ways>
class PCSetAssociativeLRUTable {
	static size_t make_set(PC pc) noexcept { return static_cast<uint64_t>(pc) % N_Sets; }
	static size_t make_tag(PC pc) noexcept { return static_cast<uint64_t>(pc) / N_Sets; }
	struct Entry {
		size_t tag;
		U value;
		bool valid;
		size_t lru_order;
	};
	std::array<std::array<Entry, N_Ways>, N_Sets> table;
	const Entry& find(PC key) const noexcept {
		const size_t set = make_set(key);
		const size_t tag = make_tag(key);
		return *std::find_if(table.at(set).begin(), table.at(set).end(), [tag](const Entry& entry) noexcept { return entry.valid && entry.tag == tag; });
	}
	Entry& find(PC key) noexcept {;
		const size_t set = make_set(key);
		const size_t tag = make_tag(key);
		return *std::find_if(table.at(set).begin(), table.at(set).end(), [tag](const Entry& entry) noexcept { return entry.valid && entry.tag == tag; });
	}
public:
	PCSetAssociativeLRUTable() : table{} {
		for (size_t i = 0; i < N_Sets; ++i) {
			for (size_t j = 0; j < N_Ways; ++j) {
				table[i][j].lru_order = j;
			}
		}
	}
	const U& operator[](PC key) const { return find(key).value; }
	U& operator[](PC key) { return find(key).value; }
	void touch(PC key) noexcept {
		const size_t set = make_set(key);
		auto& entry = find(key);
		for (auto& e : table.at(set)) {
			if (e.lru_order < entry.lru_order) {
				++e.lru_order;
			}
		}
		entry.lru_order = 0;
	}
	void allocate_or_touch(PC key) {
		if (contains(key)) {
			touch(key);
		} else {
			insert(key, U{});
		}
	}
	void insert(PC key, U elem) {
		if (contains(key)) {
			find(key).value = std::move(elem);
			touch(key);
		} else {
			const size_t set = make_set(key);
			const size_t tag = make_tag(key);
			const auto lru_pos = std::max_element(table.at(set).begin(), table.at(set).end(), [](const Entry& lhs, const Entry& rhs) noexcept { return lhs.lru_order < rhs.lru_order; });
			lru_pos->tag = tag;
			lru_pos->value = std::move(elem);
			lru_pos->valid = true;
			touch(key);
		}
	}
	bool contains(PC key) const noexcept {
		const size_t set = make_set(key);
		const size_t tag = make_tag(key);
		return std::any_of(table.at(set).begin(), table.at(set).end(), [tag](const Entry& entry) noexcept { return entry.valid && entry.tag == tag; });
	}
};

// ----------------
//  Confidence set
// ----------------
// This template is a N entries set with confidence counter. This set
// holds confidence numerator (per entry) and denominator. The maximum
// count of numerator is MaxConfidence - 1, and the maximum count of
// denominator is MaxConfidence * N - 1. adjust() function halves all the
// entries when there are any entries exceed maxmum count.
template<class T, size_t MaxConfidence, size_t N>
class ConfidenceSet {
	struct Entry {
		T key;
		size_t confidence;
	};
	std::array<Entry, N> table;
	size_t denominator;

	void adjust() noexcept {
		const bool denominator_overflow = denominator == MaxConfidence * N;
		const bool numerator_overflow = std::any_of(begin(), end(), [](const Entry& entry) { return entry.confidence == MaxConfidence; });
		if (denominator_overflow || numerator_overflow) {
			for (auto& entry : table) {
				entry.confidence >>= 1;
			}
			denominator >>= 1;
		}
	}
public:
	ConfidenceSet() : table{} {}
	const Entry* begin() const { return table.begin(); }
	Entry* begin() { return table.begin(); }
	const Entry* end() const { return table.end(); }
	Entry* end() { return table.end(); }
	void count_up(const T& tag) {
		const auto existing_entry = std::find_if(begin(), end(), [&tag](const Entry& entry) { return entry.confidence > 0 && entry.key == tag; });
		if (existing_entry != end()) {
			++existing_entry->confidence;
			++denominator;
			adjust();
		} else {
			const auto invalid_entry = std::find_if(begin(), end(), [](const Entry& entry) { return entry.confidence == 0; });
			if (invalid_entry != end()) {
				invalid_entry->key = tag;
				invalid_entry->confidence = 1;
				++denominator;
				adjust();
			} else {
				++denominator;
				adjust();
			}
		}
	}
	bool contains(const T& tag) const { return std::any_of(begin(), end(), [&tag](const Entry& entry) { return entry.confidence > 0 && entry.key == tag; }); }
};

// ============
//  Step info 
// ============
// This is an entry of Step Table. This holds address of last access
// (last_addr) to calculate delta.  
struct StepInfo {
	RawAddr last_addr;
	ConfidenceSet<uint64_t, 8, 4> step_group;
	ConfidenceSet<PC, 8, 16> trigger_pc_group;;
	StepInfo() noexcept : last_addr(0), step_group{}, trigger_pc_group{} {}
	explicit StepInfo(RawAddr last_addr) noexcept : last_addr(last_addr), step_group{}, trigger_pc_group{} {}
};

// ===============
//  Triger access
// ===============
// PC and address of trigger access.
struct TriggerAccess {
	PC pc;
	RawAddr addr;
	bool operator==(const TriggerAccess& rhs) const noexcept { return pc == rhs.pc && addr == rhs.addr; }
	bool operator!=(const TriggerAccess& rhs) const noexcept { return pc != rhs.pc || addr != rhs.addr; }
};

static constexpr size_t RecentRequestTableSize = 16;
static constexpr size_t StepTableSize = 512;
static constexpr size_t TargetTableSize = 512;
static constexpr size_t TargetTable_TargetPerEntry = 16;
static constexpr size_t DELAY_QUEUE_SIZE = 128;
static constexpr uint16_t DELAY_TIME = 100;
static constexpr size_t MaxInflightPrefetch = L1D_PQ_SIZE + L1D_MSHR_SIZE; // 8+8=16



CACHE* pCache;

// ------------
//  Meta cache
// ------------
// This is a component of Prefetch Filter which denies prefetching
// addresses existing in the cache in order not to waste PQ entries.
// We made this table to obey the DPC3 rules.
// It has only tags of the caches.
// Budget:
//  tag 36bits X 8way X 64set = 18432bits
std::array<std::array<uint64_t, L1D_WAY>, L1D_SET> meta_cache[NUM_CPUS];
uint64_t make_L1D_tag(RawAddr addr) noexcept {
	return (static_cast<uint64_t>(addr) >> LOG2_BLOCK_SIZE) / L1D_SET; 
}
bool is_in_cache(RawAddr addr) noexcept {
	const size_t set = static_cast<uint64_t>(addr) >> LOG2_BLOCK_SIZE & ~-L1D_SET;
	return std::find(meta_cache[pCache->cpu].at(set).begin(), meta_cache[pCache->cpu].at(set).end(), make_L1D_tag(addr)) != meta_cache[pCache->cpu].at(set).end();
}

// --------------
//  PrefetchBit
// --------------
// This is used to know whether a cache line is inserted by a prefetcher
// when a cache hit occurs.
// Budget:
//  1bit X 8way X 64set = 512bits
std::array<std::array<bool, L1D_WAY>, L1D_SET> prefetch_bit[NUM_CPUS];
// This function returns whether a prefetch hit occurs.
// If a prefetch hit occurs, unset the prefetch bit so that second access is not regarded as a prefetch hit.
bool prefetch_bit_operate(RawAddr addr, bool cache_hit) {
	if (cache_hit) {
		const size_t set = static_cast<uint64_t>(addr) >> LOG2_BLOCK_SIZE & ~-L1D_SET;
		const size_t way = std::find(meta_cache[pCache->cpu].at(set).begin(), meta_cache[pCache->cpu].at(set).end(), make_L1D_tag(addr)) - meta_cache[pCache->cpu].at(set).begin();
		const bool ret = prefetch_bit[pCache->cpu].at(set).at(way);
		prefetch_bit[pCache->cpu].at(set).at(way) = false;
		return ret;
	} else {
		// All cache misses are not prefetch hits.
		return false;
	}
}

// ---------------------
//  RecentRequestsTable
// ---------------------
// BOP like timing learning table. 
// This is used to know what access should trigger prefetch for current access.
// Budget:
//  pc      16bit X 16entry = 256bits
//  address 48bit X 16entry = 768bits
//  lru bit  4bit X 16entry =  64bits
//                           1168bits
FullyAssociativeLRUSet<TriggerAccess, RecentRequestTableSize> recent_requests_table[NUM_CPUS];

// ---------------
//  Step table
// ---------------
// Step Table is a table to learn steps of the same PC including confidence.
// In addition, Step Table records the appropriate trigger PCs of a certain target PC.
// Note: Step table also records delta "0".
// Normal delta prefetchers do not learn delta "0", but learning of delta "0" is useful in T-SKID that controls prefetch timing.
// Budget:
//  target pc tag    12bit X 512entry =   6144bits
//  last access addr 48bit X 512entry =  24576bits
//  step         4 X 13bit X 512entry =  26624bits
//   confidenece 4 X  3bit X 512entry =   6144bits
//   denominator      5bit X 512entry =   2560bits
//  trigger pc  16 X 16bit X 512entry = 131072bits
//   confidence 16 X  3bit X 512entry =  24576bits
//   denominator      7bit X 512entry =   3584bits
//  lru bit           5bit X 512entry =   2560bits
//                                      227840bits
PCSetAssociativeLRUTable<StepInfo, StepTableSize/16, 16> step_table[NUM_CPUS];

// ------------
//  DelayQueue
// ------------
// Without the delay queue, the prefetcher would not the timing of cache fill of access 
// by cache hit. By putting access in the delay queue and inserting it into RRT, the 
// prefethcer can learn the appropriate timing for cache hit access as well.
// The main memory access latency is at least 97 cycle; We choose DELAY_TIME=100. 
// Budget:
//  pc      16bit X 128entry = 2048bits
//  address 48bit X 128entry = 6144bits
//  time    16bit X 128entry = 2048bits
//                            10240bits
DelayQueue<TriggerAccess, DELAY_QUEUE_SIZE, DELAY_TIME> delay_queue[NUM_CPUS];
void delay_queue_operate() {
	while (delay_queue[pCache->cpu].ready_to_pop(current_core_cycle[pCache->cpu])) {
		recent_requests_table[pCache->cpu].insert_or_touch(delay_queue[pCache->cpu].front());
		delay_queue[pCache->cpu].pop();
	}
}

// -----------------------
//  InflightPrefetchTable
// -----------------------
// Inflight Prefetch Table is a table that holds prefetch access addresses and their PCs until prefetch fill.
// Budget:
//  pf_line 42bit X 16entry = 672bits
//  pc      25bit X 16entry = 400bits
//  address 48bit X 16entry = 768bits
//                           1840bits
FullyAssociativeTable<RawAddr, TriggerAccess, MaxInflightPrefetch> inflight_prefetch[NUM_CPUS];

// --------------
//  TargetTable
// --------------
// Target Table links trigger PCs and target PCs
// Target Table has roughly the same information as Step Table, but Target Table is a table for reverse lookup with trigger PC as a tag.
// Budget:
//  trigger pc tag 12bit X 512entry =   6144bits
//  target pc 16 X 16bit X 512entry = 131072bits
//   lru bit  16 X  4bit X 512entry =  32768bits
//  lru bit         5bit X 512entry =   2560bits
//                                    172544bits
PCSetAssociativeLRUTable<FullyAssociativeLRUSet<PC, TargetTable_TargetPerEntry>, TargetTableSize/16, 16> target_table[NUM_CPUS];
bool prefetching(RawAddr addr) noexcept { return inflight_prefetch[pCache->cpu].contains(mask_line_offset(addr)); }

void train_trigger(PC target_pc) {
	for (const auto trigger : recent_requests_table[pCache->cpu]) {
		step_table[pCache->cpu][target_pc].trigger_pc_group.count_up(trigger.pc);
		target_table[pCache->cpu].allocate_or_touch(trigger.pc);
		target_table[pCache->cpu][trigger.pc].insert_or_touch(target_pc);
	}
}

bool train(PC pc, RawAddr addr, bool virtual_miss) {
	if (!step_table[pCache->cpu].contains(pc)) {
		if (virtual_miss) {
			step_table[pCache->cpu].insert(pc, StepInfo(addr));
			if (std::any_of(recent_requests_table[pCache->cpu].begin(), recent_requests_table[pCache->cpu].end(), [pc](const TriggerAccess& trigger) noexcept { return trigger.pc == pc; })) {
				// This is a case that this PC is in RRT. The PC will be able to trigger the same PC access. Using RRT, a step including a distance is learned.
				// This is first time to refer Step Table with this PC, so a step is not able to be calculated.
				target_table[pCache->cpu].allocate_or_touch(pc);
				target_table[pCache->cpu][pc].insert_or_touch(pc);
			} else {
				// This is a case that this PC access will be triggered by another PC. An address prediction is made like PC Stride Prefetcher.
				// Even if it is first time to come here, learning trigger PCs is possible.
				train_trigger(pc);
			}
		} else {
			return false;
		}
	} else {
		// seen before
		step_table[pCache->cpu].touch(pc);
		if (std::any_of(recent_requests_table[pCache->cpu].begin(), recent_requests_table[pCache->cpu].end(), [pc](const TriggerAccess& trigger) noexcept { return trigger.pc == pc; })) {
			// This is a case that this PC is in RRT. The PC will be able to trigger the same PC access. Using RRT, a step including a distance is learned.
			for (const auto trigger : recent_requests_table[pCache->cpu]) {
				if (trigger.pc == pc) {
					const int64_t step = addr - trigger.addr;
					if (-PAGE_SIZE < step && step < PAGE_SIZE) {
						step_table[pCache->cpu][pc].step_group.count_up(step);
					}
				}
				step_table[pCache->cpu][pc].trigger_pc_group.count_up(trigger.pc);
			}
			step_table[pCache->cpu][pc].last_addr = addr;
			target_table[pCache->cpu].allocate_or_touch(pc);
			target_table[pCache->cpu][pc].insert_or_touch(pc);
		} else {
			// This is a case that this PC access will be triggered by another PC. An address prediction is made like PC Stride Prefetcher.
			// It will be learned even if step == 0.
			const int64_t step = addr - step_table[pCache->cpu][pc].last_addr;
			step_table[pCache->cpu][pc].last_addr = addr;
			if (-PAGE_SIZE < step && step < PAGE_SIZE) {
				step_table[pCache->cpu][pc].step_group.count_up(step);
			}
			train_trigger(pc);
		}
	}
	return true;
}

bool do_prefetch(TriggerAccess trigger, PC targetPC) {
	bool triggered = false;
	const auto& targetPCInfo = step_table[pCache->cpu][targetPC];
	for (const auto entry : targetPCInfo.step_group) {
		if (entry.confidence == 0) { continue; }
		const int64_t step = entry.key;
		const RawAddr base_addr = targetPCInfo.last_addr;
		const RawAddr pf_addr = base_addr + step;
		if (is_in_the_same_page(base_addr, pf_addr) && !is_in_cache(pf_addr) && !prefetching(pf_addr)) {
			int success = pCache->prefetch_line(static_cast<uint64_t>(trigger.pc), static_cast<uint64_t>(targetPCInfo.last_addr), static_cast<uint64_t>(pf_addr), FILL_L1, 0);
			if (success) {
				inflight_prefetch[pCache->cpu].insert(mask_line_offset(pf_addr), trigger);
				triggered = true;
				count_pref++;
			}
		}
	}
	return triggered;
}

int operate(RawAddr addr, PC pc, bool cache_hit, bool prefetch_hit) {
	const bool cache_miss = !cache_hit;
	const bool virtual_miss = cache_miss || prefetch_hit;
	TriggerAccess trigger { pc, addr };

	// If the PC has not been virtual_miss recently, including this time, then update == false.
	const bool updated = train( pc, addr, virtual_miss );

	// Issue prefetch.
	// Control using confidence is not performed.
	bool triggered = false;
	if (target_table[pCache->cpu].contains(pc)) {
		auto& targetPCs = target_table[pCache->cpu][pc];
		for (auto targetPC : targetPCs) {
			if (!step_table[pCache->cpu].contains(targetPC)){
				targetPCs.move_to_lru(targetPC);
			} else {
				if (step_table[pCache->cpu][targetPC].trigger_pc_group.contains(pc)) {
					triggered |= do_prefetch(trigger, targetPC);
				} else {
					targetPCs.move_to_lru(targetPC);
				}
			}
		}
	}

	// Some prefetches were issued successfully.
	// Timing learning will be done at cache filling. 
	// So things to do with this function are over.
	if (triggered) {
		return 1;
	}

	// It was not possible to prefetch. The followings are workarounds.

	if (pCache->PQ.occupancy == pCache->PQ.SIZE) {
		// Because PQ is full, the PC and the address are not inserted to RRT.
		// The PC is not suitable as trigger since PQ may be full after next time too.
		return 0;
	}

	if (!updated) {
		// The PC whose access hits every time is not inserted to RRT.
		return 0;
	}

	// Timing learning using Next Line Prefetcher.
	const RawAddr pf_addr = addr + BLOCK_SIZE;
	if (is_in_the_same_page(addr, pf_addr) && !is_in_cache(pf_addr) && !prefetching(pf_addr)) {
		int success = pCache->prefetch_line(static_cast<uint64_t>(pc), static_cast<uint64_t>(addr), static_cast<uint64_t>(pf_addr), FILL_L1, 0);
		if (success) {
			inflight_prefetch[pCache->cpu].insert(mask_line_offset(pf_addr), trigger);
			count_pref++;
			return 1;
		}
	}

	// Timing larning via delay queue.
	delay_queue[pCache->cpu].push(trigger, current_core_cycle[pCache->cpu]);
	return 0;
}

void cache_fill(RawAddr addr, bool prefetch) {
	if (prefetching(addr)) {
		// If the addr is filled by prefetch, the information (trigger PC and its address) saved in inflight_prefetch is inserted to RRT.
		recent_requests_table[pCache->cpu].insert_or_touch(inflight_prefetch[pCache->cpu].get(mask_line_offset(addr)));
		inflight_prefetch[pCache->cpu].invalidate(mask_line_offset(addr));
	}
}

} // namespace anonymous


class IP_TABLE_L1 {
  public:
    uint64_t ip_tag;
    uint64_t last_page;                                     // last page seen by IP
    uint64_t last_cl_offset;                                // last cl offset in the 4KB page
    int64_t last_stride;                                    // last delta observed
    uint16_t ip_valid;                                      // Valid IP or not   
    int conf;                                               // CS conf
    uint16_t signature;                                     // CPLX signature
    uint16_t str_dir;                                       // stream direction
    uint16_t str_valid;                                     // stream valid
    uint16_t str_strength;                                  // stream strength
    int degree;                                             // prefetch degree
    int useful;                                             // useful prefetches for this ip
    int useless;                                            // useless prefetches for this ip

    IP_TABLE_L1 () {
        ip_tag = 0;
        last_page = 0;
        last_cl_offset = 0;
        last_stride = 0;
        ip_valid = 0;
        signature = 0;
        conf = 0;
        str_dir = 0;
        str_valid = 0;
        str_strength = 0;
        degree = PREFETCH_DEGREE;
        useful = 0;
        useless = 0;
    };
};

class DELTA_PRED_TABLE {
public:
    int delta;
    int conf;

    DELTA_PRED_TABLE () {
        delta = 0;
        conf = 0;
    };        
};


IP_TABLE_L1 trackers_l1[NUM_CPUS][NUM_IP_TABLE_L1_ENTRIES];
DELTA_PRED_TABLE DPT_l1[NUM_CPUS][4096];
uint64_t ghb_l1[NUM_CPUS][NUM_GHB_ENTRIES];
uint64_t prev_cpu_cycle[NUM_CPUS];
uint64_t num_misses[NUM_CPUS];
float mpkc[NUM_CPUS] = {0};
int spec_nl[NUM_CPUS] = {0};


/***************Updating the signature*************************************/ 
uint16_t update_sig_l1(uint16_t old_sig, int delta){                           
    uint16_t new_sig = 0;
    int sig_delta = 0;

// 7-bit sign magnitude form, since we need to track deltas from +63 to -63
    sig_delta = (delta < 0) ? (((-1) * delta) + (1 << 6)) : delta;
    new_sig = ((old_sig << 1) ^ sig_delta) & 0xFFF;                     // 12-bit signature

    return new_sig;
}



/****************Encoding the metadata***********************************/
uint32_t encode_metadata(int stride, uint16_t type, int spec_nl){

uint32_t metadata = 0;

// first encode stride in the last 8 bits of the metadata
if(stride > 0)
    metadata = stride;
else
    metadata = ((-1*stride) | 0b1000000);

// encode the type of IP in the next 4 bits
metadata = metadata | (type << 8);

// encode the speculative NL bit in the next 1 bit
metadata = metadata | (spec_nl << 12);

return metadata;

}


/*********************Checking for a global stream (GS class)***************/

void check_for_stream_l1(int index, uint64_t cl_addr, uint8_t cpu){
int pos_count=0, neg_count=0, count=0;
uint64_t check_addr = cl_addr;

// check for +ve stream
    for(int i=0; i<NUM_GHB_ENTRIES; i++){
        check_addr--;
        for(int j=0; j<NUM_GHB_ENTRIES; j++)
            if(check_addr == ghb_l1[cpu][j]){
                pos_count++;
                break;
            }
    }

check_addr = cl_addr;
// check for -ve stream
    for(int i=0; i<NUM_GHB_ENTRIES; i++){
        check_addr++;
        for(int j=0; j<NUM_GHB_ENTRIES; j++)
            if(check_addr == ghb_l1[cpu][j]){
                neg_count++;
                break;
            }
    }

    if(pos_count > neg_count){                                // stream direction is +ve
        trackers_l1[cpu][index].str_dir = 1;
        count = pos_count;
    }
    else{                                                     // stream direction is -ve
        trackers_l1[cpu][index].str_dir = 0;
        count = neg_count;
    }

if(count > NUM_GHB_ENTRIES/2){                                // stream is detected
    trackers_l1[cpu][index].str_valid = 1;
    if(count >= (NUM_GHB_ENTRIES*3)/4)                        // stream is classified as strong if more than 3/4th entries belong to stream
        trackers_l1[cpu][index].str_strength = 1;
}
else{
    if(trackers_l1[cpu][index].str_strength == 0)             // if identified as weak stream, we need to reset
        trackers_l1[cpu][index].str_valid = 0;
}

}

/**************************Updating confidence for the CS class****************/
int update_conf(int stride, int pred_stride, int conf){
    if(stride == pred_stride){             // use 2-bit saturating counter for confidence
        conf++;
        if(conf > 3)
            conf = 3;
    } else {
        conf--;
        if(conf < 0)
            conf = 0;
    }

return conf;
}

/*************************throttle prefetch based on accuracy******************/

void l1d_prefetch_throttle(int index,int cpu){
    if(trackers_l1[cpu][index].useful+trackers_l1[cpu][index].useless >4){
        float accuracy = (1.0*trackers_l1[cpu][index].useful)/(trackers_l1[cpu][index].useful+trackers_l1[cpu][index].useless);
        if(accuracy >= AC_HIGH){ // INCREASE PREFETCH_DEGREE TO INCREASE TIMELINESS
            trackers_l1[cpu][index].degree++;
        }
        if(accuracy < AC_HIGH && accuracy >=AC_LOW){ // IDEAL CASE
            trackers_l1[cpu][index].degree;  
        }
        if(accuracy < AC_LOW){ // DECREASE PREFETCH_DEGREE TO SAVE BANDWIDTH
            trackers_l1[cpu][index].degree--; 
        }
        if(trackers_l1[cpu][index].degree<=0) {
            trackers_l1[cpu][index].degree = 1;
        }
        if(trackers_l1[cpu][index].degree>THRESHOLD){
            trackers_l1[cpu][index].degree = THRESHOLD;
        }
    }
}

void CACHE::l1d_prefetch_update(uint64_t ip,int type){
    int index = ip & ((1 << NUM_IP_INDEX_BITS)-1);
    uint16_t ip_tag = (ip >> NUM_IP_INDEX_BITS) & ((1 << NUM_IP_TAG_BITS)-1);
    if(trackers_l1[cpu][index].ip_tag == ip_tag){  // existing IP
            if(type==0){ // UPDATE USEFUL 
                trackers_l1[cpu][index].useful++;
            }
            if(type==1){ // UPDATE USELESS 
                trackers_l1[cpu][index].useless++;
            }
            l1d_prefetch_throttle(index,cpu); // CALL PREFETCH_THROTTLE TO CHANGE THE PREFETCH_DEGREE
    }
}

void CACHE::l1d_prefetcher_initialize() 
{

}

void CACHE::l1d_prefetcher_operate(uint64_t addr, uint64_t ip, uint8_t cache_hit, uint8_t type)
{
	count_pref = 0;
    uint64_t curr_page = addr >> LOG2_PAGE_SIZE;
    uint64_t cl_addr = addr >> LOG2_BLOCK_SIZE;
    uint64_t cl_offset = (addr >> LOG2_BLOCK_SIZE) & 0x3F;
    uint16_t signature = 0, last_signature = 0;
    int prefetch_degree = 0;
    int spec_nl_threshold = 0;
    int num_prefs = 0;
    uint32_t metadata=0;
    uint16_t ip_tag = (ip >> NUM_IP_INDEX_BITS) & ((1 << NUM_IP_TAG_BITS)-1);

// update miss counter
if(cache_hit == 0)
    num_misses[cpu] += 1;
if(NUM_CPUS == 1){
    spec_nl_threshold = 15; 
} else {                                    // tightening the degree and MPKC constraints for multi-core
    spec_nl_threshold = 5;
}

// update spec nl bit when num misses crosses certain threshold
if(num_misses[cpu] == 256){
    mpkc[cpu] = ((float) num_misses[cpu]/(current_core_cycle[cpu]-prev_cpu_cycle[cpu]))*1000;
    prev_cpu_cycle[cpu] = current_core_cycle[cpu];
    if(mpkc[cpu] > spec_nl_threshold)
        spec_nl[cpu] = 0;
    else
        spec_nl[cpu] = 1;
    num_misses[cpu] = 0;
}

// calculate the index bit
    int index = ip & ((1 << NUM_IP_INDEX_BITS)-1);
    if(trackers_l1[cpu][index].ip_tag != ip_tag){               // new/conflict IP
        if(trackers_l1[cpu][index].ip_valid == 0){              // if valid bit is zero, update with latest IP info
        trackers_l1[cpu][index].ip_tag = ip_tag;
        trackers_l1[cpu][index].last_page = curr_page;
        trackers_l1[cpu][index].last_cl_offset = cl_offset;
        trackers_l1[cpu][index].last_stride = 0;
        trackers_l1[cpu][index].signature = 0;
        trackers_l1[cpu][index].conf = 0;
        trackers_l1[cpu][index].str_valid = 0;
        trackers_l1[cpu][index].str_strength = 0;
        trackers_l1[cpu][index].str_dir = 0;
        trackers_l1[cpu][index].ip_valid = 1;
    } else {                                                    // otherwise, reset valid bit and leave the previous IP as it is
        trackers_l1[cpu][index].ip_valid = 0;
    }

    // issue a next line prefetch upon encountering new IP
        uint64_t pf_address = ((addr>>LOG2_BLOCK_SIZE)+1) << LOG2_BLOCK_SIZE; // BASE NL=1, changing it to 3
        metadata = encode_metadata(1, NL_TYPE, spec_nl[cpu]);
        prefetch_line(ip, addr, pf_address, FILL_L1, metadata);
        return;
    }
    else {                                                     // if same IP encountered, set valid bit
        trackers_l1[cpu][index].ip_valid = 1;
    }
    

    // calculate the stride between the current address and the last address
    int64_t stride = 0;
    if (cl_offset > trackers_l1[cpu][index].last_cl_offset)
        stride = cl_offset - trackers_l1[cpu][index].last_cl_offset;
    else {
        stride = trackers_l1[cpu][index].last_cl_offset - cl_offset;
        stride *= -1;
    }

    // don't do anything if same address is seen twice in a row
    if (stride == 0)
        return;


// page boundary learning
if(curr_page != trackers_l1[cpu][index].last_page){
    if(stride < 0)
        stride += 64;
    else
        stride -= 64;
}

// update constant stride(CS) confidence
trackers_l1[cpu][index].conf = update_conf(stride, trackers_l1[cpu][index].last_stride, trackers_l1[cpu][index].conf);

// update CS only if confidence is zero
if(trackers_l1[cpu][index].conf == 0)                      
    trackers_l1[cpu][index].last_stride = stride;

last_signature = trackers_l1[cpu][index].signature;
// update complex stride(CPLX) confidence
DPT_l1[cpu][last_signature].conf = update_conf(stride, DPT_l1[cpu][last_signature].delta, DPT_l1[cpu][last_signature].conf);

// update CPLX only if confidence is zero
if(DPT_l1[cpu][last_signature].conf == 0)
    DPT_l1[cpu][last_signature].delta = stride;

// calculate and update new signature in IP table
signature = update_sig_l1(last_signature, stride);
trackers_l1[cpu][index].signature = signature;

// check GHB for stream IP
check_for_stream_l1(index, cl_addr, cpu);    

if(NUM_CPUS == 1){
    prefetch_degree = trackers_l1[cpu][index].degree;
} else {                                    // tightening the degree and MPKC constraints for multi-core
    prefetch_degree = 2;
}

SIG_DP(
cout << ip << ", " << cache_hit << ", " << cl_addr << ", " << addr << ", " << stride << "; ";
cout << last_signature<< ", "  << DPT_l1[cpu][last_signature].delta<< ", "  << DPT_l1[cpu][last_signature].conf << "; ";
cout << trackers_l1[cpu][index].last_stride << ", " << stride << ", " << trackers_l1[cpu][index].conf << ", " << "; ";
);

    if(trackers_l1[cpu][index].str_valid == 1){                         // stream IP
        // for stream, prefetch with twice the usual degree
            prefetch_degree = prefetch_degree*2;
        for (int i=0; i<prefetch_degree; i++) {
            uint64_t pf_address = 0;

            if(trackers_l1[cpu][index].str_dir == 1){                   // +ve stream
                pf_address = (cl_addr + i + 1) << LOG2_BLOCK_SIZE;
                metadata = encode_metadata(1, S_TYPE, spec_nl[cpu]);    // stride is 1
            }
            else{                                                       // -ve stream
                pf_address = (cl_addr - i - 1) << LOG2_BLOCK_SIZE;
                metadata = encode_metadata(-1, S_TYPE, spec_nl[cpu]);   // stride is -1
            }

            // Check if prefetch address is in same 4 KB page
            if ((pf_address >> LOG2_PAGE_SIZE) != (addr >> LOG2_PAGE_SIZE)){
                break;
            }

            prefetch_line(ip, addr, pf_address, FILL_L1, metadata);
            num_prefs++;
            SIG_DP(cout << "1, ");
            }

    } else if(trackers_l1[cpu][index].conf > 1 && trackers_l1[cpu][index].last_stride != 0){            // CS IP  
        for (int i=0; i<prefetch_degree; i++) {
            uint64_t pf_address = (cl_addr + (trackers_l1[cpu][index].last_stride*(i+1))) << LOG2_BLOCK_SIZE;

            // Check if prefetch address is in same 4 KB page
            if ((pf_address >> LOG2_PAGE_SIZE) != (addr >> LOG2_PAGE_SIZE)){
                break;
            }

            metadata = encode_metadata(trackers_l1[cpu][index].last_stride, CS_TYPE, spec_nl[cpu]);
            prefetch_line(ip, addr, pf_address, FILL_L1, metadata);
            num_prefs++;
            SIG_DP(cout << trackers_l1[cpu][index].last_stride << ", ");
        }
    } else if(DPT_l1[cpu][signature].conf >= 0 && DPT_l1[cpu][signature].delta != 0) {  // if conf>=0, continue looking for delta
        int pref_offset = 0,i=0;                                                        // CPLX IP
        for (i=0; i<prefetch_degree; i++) {
            pref_offset += DPT_l1[cpu][signature].delta;
            uint64_t pf_address = ((cl_addr + pref_offset) << LOG2_BLOCK_SIZE);

            // Check if prefetch address is in same 4 KB page
            if (((pf_address >> LOG2_PAGE_SIZE) != (addr >> LOG2_PAGE_SIZE)) || 
                    (DPT_l1[cpu][signature].conf == -1) ||
                    (DPT_l1[cpu][signature].delta == 0)){
                // if new entry in DPT or delta is zero, break
                break;
            }

            // we are not prefetching at L2 for CPLX type, so encode delta as 0
            metadata = encode_metadata(0, CPLX_TYPE, spec_nl[cpu]);
            if(DPT_l1[cpu][signature].conf > 0){                                 // prefetch only when conf>0 for CPLX
                prefetch_line(ip, addr, pf_address, FILL_L1, metadata);
                num_prefs++;
                SIG_DP(cout << pref_offset << ", ");
            }
            signature = update_sig_l1(signature, DPT_l1[cpu][signature].delta);
        }
    } 
if(num_prefs==0){
	pCache = this;
		const bool prefetch_hit = ::prefetch_bit_operate(RawAddr(addr), cache_hit);
		::delay_queue_operate();
		::operate(RawAddr(addr), PC(ip), cache_hit, prefetch_hit);
}

// if no prefetches are issued till now, speculatively issue a next_line prefetch
if(num_prefs == 0 && spec_nl[cpu] == 1 && count_pref == 0){                                        // NL IP
    uint64_t pf_address = ((addr>>LOG2_BLOCK_SIZE)+1) << LOG2_BLOCK_SIZE;  
    metadata = encode_metadata(1, NL_TYPE, spec_nl[cpu]);
    prefetch_line(ip, addr, pf_address, FILL_L1, metadata);
    SIG_DP(cout << "1, ");
}

SIG_DP(cout << endl);

// update the IP table entries
trackers_l1[cpu][index].last_cl_offset = cl_offset;
trackers_l1[cpu][index].last_page = curr_page;

// update GHB
// search for matching cl addr
int ghb_index=0;
for(ghb_index = 0; ghb_index < NUM_GHB_ENTRIES; ghb_index++)
    if(cl_addr == ghb_l1[cpu][ghb_index])
        break;
// only update the GHB upon finding a new cl address
if(ghb_index == NUM_GHB_ENTRIES){
for(ghb_index=NUM_GHB_ENTRIES-1; ghb_index>0; ghb_index--)
    ghb_l1[cpu][ghb_index] = ghb_l1[cpu][ghb_index-1];
ghb_l1[cpu][0] = cl_addr;
}

return;
}

void CACHE::l1d_prefetcher_cache_fill(uint64_t addr, uint32_t set, uint32_t way, uint8_t prefetch, uint64_t evicted_addr, uint32_t metadata_in)
{
    pCache = this;
	::meta_cache[pCache->cpu].at(set).at(way) = ::make_L1D_tag(RawAddr(addr));
	::prefetch_bit[pCache->cpu].at(set).at(way) = static_cast<bool>(prefetch);
	::delay_queue_operate();
	::cache_fill(RawAddr(addr), prefetch);
}
void CACHE::l1d_prefetcher_final_stats()
{
cout << endl;
}

